{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Control Input: [[296011.57180216]]\n",
      "Total Cost: [[8.76228506e+12]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganga\\AppData\\Local\\Temp\\ipykernel_6328\\4182227672.py:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  position, velocity = float(x[0]), float(x[1])  # Ensure scalars\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from scipy.linalg import solve_continuous_are\n",
    "import time\n",
    "\n",
    "# Define the dynamics and cost functions\n",
    "def dynamics(x, u):\n",
    "    \"\"\"\n",
    "    Dynamics of the Continuous Mountain Car environment.\n",
    "    x: state vector [position, velocity]\n",
    "    u: control input [force]\n",
    "    \"\"\"\n",
    "    position, velocity = x\n",
    "    force = np.clip(u, -1.0, 1.0)  # Clip force to valid range\n",
    "    new_velocity = velocity + 0.001 * force - 0.0025 * np.cos(3 * position)\n",
    "    new_position = position + new_velocity\n",
    "    new_velocity = np.clip(new_velocity, -0.07, 0.07)  # Clip velocity to valid range\n",
    "    return np.array([new_position, new_velocity])\n",
    "\n",
    "def cost(x, u):\n",
    "    \"\"\"\n",
    "    Cost function for the Continuous Mountain Car problem.\n",
    "    x: state vector [position, velocity]\n",
    "    u: control input [force]\n",
    "    \"\"\"\n",
    "    position, velocity = x\n",
    "    target_position = 0.45  # Target position (top of the hill)\n",
    "    return (position - target_position) ** 2 + 0.1 * (u ** 2)  # Quadratic cost\n",
    "\n",
    "def linearize_dynamics(x, u):\n",
    "    \"\"\"\n",
    "    Linearize the dynamics around the current state and control.\n",
    "    Returns A (state Jacobian) and B (control Jacobian).\n",
    "    \"\"\"\n",
    "    position, velocity = float(x[0]), float(x[1])  # Ensure scalars\n",
    "    A = np.array([[1, 0.001], [0.0025 * 3 * np.sin(3 * position), 1]], dtype=np.float64)\n",
    "    B = np.array([[0], [0.001]], dtype=np.float64)\n",
    "    return A, B\n",
    "\n",
    "def iLQR(env, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Iterative Linear Quadratic Regulator (iLQR) algorithm.\n",
    "    \"\"\"\n",
    "    # Initialize state and control\n",
    "    state = env.reset()[0]\n",
    "    u = np.zeros(1)  # Initial control input\n",
    "\n",
    "    # Define cost matrices\n",
    "    Q = np.diag([1.0, 0.1])  # State cost\n",
    "    R = np.array([[0.1]])  # Control cost\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Linearize dynamics\n",
    "        A, B = linearize_dynamics(state, u)\n",
    "\n",
    "        # Solve Riccati equation for the value function\n",
    "        P = solve_continuous_are(A, B, Q, R)\n",
    "\n",
    "        # Compute optimal control\n",
    "        K = -np.linalg.inv(R + B.T @ P @ B) @ B.T @ P @ A\n",
    "        u_new = K @ state.reshape(-1, 1)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(u_new - u) < tol:\n",
    "            break\n",
    "\n",
    "        # Update control\n",
    "        u = u_new\n",
    "\n",
    "        # Simulate one step\n",
    "        state = dynamics(state, u)\n",
    "\n",
    "    return u\n",
    "\n",
    "# Main function to run the iLQR algorithm\n",
    "def main():\n",
    "    env = gym.make(\"MountainCarContinuous-v0\")\n",
    "    optimal_control = iLQR(env)\n",
    "    print(\"Optimal Control Input:\", optimal_control)\n",
    "\n",
    "    # Test the optimal control\n",
    "    state = env.reset()[0]\n",
    "    total_cost = 0\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "        #time.sleep(0.01)  # Slow down for visibility\n",
    "        state = dynamics(state, optimal_control)\n",
    "        total_cost += cost(state, optimal_control)\n",
    "        if state[0] >= 0.45:  # Check if the car reached the goal\n",
    "            print(\"Goal reached!\")\n",
    "            break\n",
    "    print(\"Total Cost:\", total_cost)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53583056  0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "state = env.reset()[0]\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from scipy.linalg import solve_continuous_are\n",
    "\n",
    "# Define the dynamics and cost functions\n",
    "def dynamics(x, u):\n",
    "    \"\"\"\n",
    "    Dynamics of the Continuous Mountain Car environment.\n",
    "    x: state vector [position, velocity]\n",
    "    u: control input [force]\n",
    "    \"\"\"\n",
    "    position, velocity = x\n",
    "    force = np.clip(u, -1.0, 1.0)  # Clip force to valid range\n",
    "    new_velocity = velocity + 0.001 * force - 0.0025 * np.cos(3 * position)\n",
    "    new_position = position + new_velocity\n",
    "    new_velocity = np.clip(new_velocity, -0.07, 0.07)  # Clip velocity to valid range\n",
    "    return np.array([new_position, new_velocity])\n",
    "\n",
    "def cost(x, u):\n",
    "    \"\"\"\n",
    "    Cost function for the Continuous Mountain Car problem.\n",
    "    x: state vector [position, velocity]\n",
    "    u: control input [force]\n",
    "    \"\"\"\n",
    "    position, velocity = x\n",
    "    target_position = 0.45  # Target position (top of the hill)\n",
    "    return (position - target_position) ** 2 + 0.1 * (u ** 2)  # Quadratic cost\n",
    "\n",
    "def linearize_dynamics(x, u):\n",
    "    \"\"\"\n",
    "    Linearize the dynamics around the current state and control.\n",
    "    Returns A (state Jacobian) and B (control Jacobian).\n",
    "    \"\"\"\n",
    "    position, velocity = x\n",
    "    A = np.array([[1, 0.001], [0.0025 * 3 * np.sin(3 * position), 1]])\n",
    "    B = np.array([[0], [0.001]])\n",
    "    return A, B\n",
    "\n",
    "def iLQR(env, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Iterative Linear Quadratic Regulator (iLQR) algorithm.\n",
    "    \"\"\"\n",
    "    # Initialize state and control\n",
    "    state = env.reset()[0]\n",
    "    u = np.zeros(1)  # Initial control input\n",
    "\n",
    "    # Define cost matrices\n",
    "    Q = np.diag([1.0, 0.1])  # State cost\n",
    "    R = np.array([[0.1]])  # Control cost\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Linearize dynamics\n",
    "        A, B = linearize_dynamics(state, u)\n",
    "\n",
    "        # Solve Riccati equation for the value function\n",
    "        P = solve_continuous_are(A, B, Q, R)\n",
    "\n",
    "        # Compute optimal control\n",
    "        K = -np.linalg.inv(R + B.T @ P @ B) @ B.T @ P @ A\n",
    "        u_new = K @ state\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(u_new - u) < tol:\n",
    "            break\n",
    "\n",
    "        # Update control\n",
    "        u = u_new\n",
    "\n",
    "        # Simulate one step\n",
    "        state = dynamics(state, u)\n",
    "\n",
    "    return u\n",
    "\n",
    "# Main function to run the iLQR algorithm\n",
    "def main():\n",
    "    env = gym.make(\"MountainCarContinuous-v0\")\n",
    "    optimal_control = iLQR(env)\n",
    "    print(\"Optimal Control Input:\", optimal_control)\n",
    "\n",
    "    # Test the optimal control\n",
    "    state = env.reset()[0]\n",
    "    total_cost = 0\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "        state = dynamics(state, optimal_control)\n",
    "        total_cost += cost(state, optimal_control)\n",
    "        if state[0] >= 0.45:  # Check if the car reached the goal\n",
    "            print(\"Goal reached!\")\n",
    "            break\n",
    "    print(\"Total Cost:\", total_cost)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
